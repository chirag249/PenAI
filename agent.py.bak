#!/usr/bin/env python3
"""
Orchestrator agent (robust, tolerant, integrates adapters/tools).

Place this file at the project root (overwrite existing agent.py).

Usage:
  python3 agent.py --targets https://example.com --run-id myrun [--mode destructive]
                   [--poc-multi] [--poc-threshold 0.5] [--no-html] [--no-pdf]
                   [--force-destructive]  # explicit bypass of destructive safeguards
"""
from __future__ import annotations
import asyncio
import argparse
import os
import sys
import importlib.util
import json
import logging
from typing import List, Dict, Any, Tuple, Optional

# project modules (expected paths)
from modules.scope import ScopeManager
from modules.logger import init_logger
from modules.reporter import Reporter
from modules.recon.passive import passive_recon
from modules.recon.active import active_recon
from modules.scanner.xss import xss_check
from modules.scanner.sqli import sqli_check
from modules.triage import triage_all
from modules.crawler import crawl
from modules.fingerprint import fingerprint
from modules.scanner.param_discovery import discover_params
from modules.scanner.form_tester import test_all_params
from modules.poc.runner import generate_pocs_for_findings
# normalize/map/attach PoCs (we'll call safely)
from modules.poc import normalize_pocs as normalize_pocs_module
from modules.poc import map_pocs_to_findings as map_pocs_module
from modules.poc import attach_pocs as attach_pocs_module
# centralized tool envelope parser
from modules.tools.parsers import parse_tool_envelope
from modules.parsers.wpscan_parser import parse_wpscan_file
from modules.parsers.nmap_parser import parse_nmap_file

# Try to import run_tool from modules.tools.manager if available
try:
    from modules.tools.manager import run_tool  # type: ignore
except Exception:
    run_tool = None  # type: ignore

# dynamic load of reporter/generate_curated.py (keeps it invokable as a module)
_gen_path = os.path.join(os.path.dirname(__file__), "modules", "reporter", "generate_curated.py")
gen_mod = None
if os.path.exists(_gen_path):
    try:
        spec = importlib.util.spec_from_file_location("generate_curated", _gen_path)
        gen_mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(gen_mod)  # type: ignore
    except Exception:
        gen_mod = None

# ---------------- adapters configuration ----------------
# (tool_name, is_destructive_capable)
TOOL_ADAPTERS: List[Tuple[str, bool]] = [
    ("sqlmap", True),     # exploit capable
    ("nmap", False),      # network scanner
    ("wpscan", True),     # can brute force, flagged destructive
    ("nikto", False),     # safe web scanner
    ("nuclei", False),    # template-based, safe by default
    ("wapiti", False),    # web vuln scanner, non-destructive mode
]

# ---------------- helpers ----------------
def safe_call_module_fn(module, possible_names: List[str], *args, **kwargs):
    """
    Try to call any of possible_names on module. Return result or None.
    """
    if module is None:
        return None
    for name in possible_names:
        fn = getattr(module, name, None)
        if callable(fn):
            return fn(*args, **kwargs)
    return None

def write_json(path: str, obj: Any) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as fh:
        json.dump(obj, fh, indent=2, ensure_ascii=False)

def parse_tool_outputs(run_dir: str) -> List[Dict[str, Any]]:
    """
    Read generated/tools/*.json in run_dir and return normalized findings list
    by delegating to modules.tools.parsers.parse_tool_envelope.
    """
    findings: List[Dict[str, Any]] = []
    gen_dir = os.path.join(run_dir, "generated", "tools")
    if not os.path.isdir(gen_dir):
        return findings

    for fname in os.listdir(gen_dir):
        if not fname.endswith(".json"):
            continue
        path = os.path.join(gen_dir, fname)
        try:
            with open(path, "r", encoding="utf-8") as fh:
                envelope = json.load(fh)
        except Exception:
            findings.append({
                "type": f"external-tool-{fname}",
                "target": "<unknown>",
                "severity": 1,
                "evidence": f"failed to read {fname}",
                "source": {"file": path}
            })
            continue

        tool_name = fname.rsplit(".", 1)[0]
        # Use parser dispatcher
        try:
            parsed = parse_tool_envelope(tool_name, envelope, run_dir)
        except Exception:
            parsed = None
        if parsed:
            for p in parsed:
                # ensure source/tool metadata
                if isinstance(p, dict):
                    p.setdefault("source", {}).setdefault("tool", tool_name)
            findings.extend(parsed)
        else:
            # fallback generic envelope wrap
            findings.append({
                "type": f"external-tool-{tool_name}",
                "target": envelope.get("result", {}).get("target") if isinstance(envelope, dict) else "<unknown>",
                "severity": 2,
                "evidence": json.dumps(envelope)[:1500],
                "source": {"tool": tool_name, "file": os.path.relpath(path, run_dir)},
                "raw_tool_output": envelope,
            })
    return findings

def run_external_tools(scope: ScopeManager, outdir: str, logger, only_non_destructive: bool = True) -> List[str]:
    """
    Run adapter list using run_tool or fallbacks. Writes outputs under runs/.../generated/tools/<tool>.json.
    Returns list of files written.
    """
    written = []
    gen_dir = os.path.join(outdir, "generated", "tools")
    os.makedirs(gen_dir, exist_ok=True)

    for tool_name, is_destructive in TOOL_ADAPTERS:
        if is_destructive and only_non_destructive:
            logger.info("Skipping destructive-capable adapter %s (safety gate)", tool_name)
            out_path = os.path.join(gen_dir, f"{tool_name}.json")
            write_json(out_path, {"status": "skipped_by_safety", "tool": tool_name})
            written.append(out_path)
            continue

        logger.info("Running adapter: %s (destructive=%s)", tool_name, is_destructive)
        # Preferred: call modules.tools.<tool>_adapter.run if exists
        adapter_module = None
        adapter_path = os.path.join(os.path.dirname(__file__), "modules", "tools", f"{tool_name}_adapter.py")
        try:
            if os.path.exists(adapter_path):
                spec = importlib.util.spec_from_file_location(f"modules.tools.{tool_name}_adapter", adapter_path)
                adapter_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(adapter_module)  # type: ignore
        except Exception as e:
            logger.debug("Adapter module import failed for %s: %s", tool_name, e)
            adapter_module = None

        result = None
        # try adapter.run(outdir, target) pattern
        try:
            if adapter_module:
                # if adapter exposes run(outdir, target) or similar, prefer it
                meta = {}
                try:
                    with open(os.path.join(outdir, "run_meta.json"), "r", encoding="utf-8") as fh:
                        meta = json.load(fh)
                except Exception:
                    meta = {}
                targets = meta.get("targets") or meta.get("target") or []
                t0 = targets[0] if isinstance(targets, list) and targets else (targets if isinstance(targets, str) else None)
                # call run(outdir, target) or run(outdir)
                result = safe_call_module_fn(adapter_module, ["run", "main", "run_adapter"], outdir, t0)
            elif run_tool:
                # fallback to manager.run_tool(tool_name, outdir)
                result = run_tool(tool_name, outdir)
            else:
                result = {"status": "not_available", "tool": tool_name, "note": "no adapter/module and no run_tool"}
        except Exception as e:
            logger.exception("Adapter %s execution failed: %s", tool_name, e)
            result = {"status": "error", "tool": tool_name, "error": str(e)}

        out_path = os.path.join(gen_dir, f"{tool_name}.json")
        try:
            write_json(out_path, result)
        except Exception:
            # ensure at least raw string
            try:
                with open(out_path, "w", encoding="utf-8") as fh:
                    fh.write(str(result))
            except Exception:
                pass
        written.append(out_path)
        logger.info("Adapter %s: wrote %s", tool_name, out_path)
    return written

# ---------------- main phases ----------------
async def run_non_destructive_phase(scope: ScopeManager, outdir: str,
                                    poc_best_only: bool = True, poc_threshold: float = 0.5,
                                    export_html: bool = True, export_pdf: bool = True) -> dict:
    logger = init_logger(outdir)
    logger.info("Starting non-destructive phase")

    # Passive recon
    logger.info("Starting passive recon")
    passive = await passive_recon(scope, outdir)
    logger.info("Passive recon done")

    # Active recon
    logger.info("Starting active recon")
    active = await active_recon(scope, outdir)
    logger.info("Active recon done")

    # Crawl
    logger.info("Starting crawler")
    urls = await crawl(scope, outdir)
    logger.info("Crawl done, found %s pages", len(urls))

    # Fingerprint
    logger.info("Starting fingerprint")
    fp = await fingerprint(scope, outdir)
    logger.info("Fingerprint done")

    # Param discovery & form testing
    logger.info("Discovering parameterized endpoints")
    params = await discover_params(scope, outdir)
    logger.info("Discovered %s parameterized endpoints", len(params))

    logger.info("Testing discovered parameter endpoints (forms & queries)")
    form_findings = await test_all_params(outdir)
    logger.info("Form testing done, results: %s", len(form_findings))

    # Optionally run external adapters (safety gated)
    try:
        only_non_destructive = not scope.is_destructive_allowed(outdir)
        logger.info("Running external adapters (only_non_destructive=%s)", only_non_destructive)
        run_external_tools(scope, outdir, logger, only_non_destructive)
    except Exception as e:
        logger.exception("external adapters step failed: %s", e)

    # Build scan target list
    targets_to_scan: List[str] = list(scope.targets)
    for p in params:
        if p.get("type") in ("query", "form"):
            targets_to_scan.append(p["url"])

    # Run built-in scanners
    findings: List[Dict[str, Any]] = []
    logger.info("Targets to scan: %s", len(targets_to_scan))
    for url in targets_to_scan:
        logger.info("Scanning %s for XSS", url)
        try:
            findings += await xss_check(url, outdir)
        except Exception:
            logger.exception("xss_check failed for %s", url)
        logger.info("Scanning %s for SQLi", url)
        try:
            findings += await sqli_check(url, outdir)
        except Exception:
            logger.exception("sqli_check failed for %s", url)

    # Add form findings
    findings.extend(form_findings)

    # Parse any generated tool outputs via centralized parser
    # After run_external_tools(...) and before triage_all(...)
    try:
        parsed_tools = parse_tool_outputs(outdir)
        if parsed_tools:
            findings.extend(parsed_tools)
            logger.info("Tool parsers: added %s findings from generated/tools", len(parsed_tools))
        else:
            logger.debug("Tool parsers: no findings parsed from generated/tools")
    except Exception:
        logger.exception("tool parsing failed")

    # Triage
    findings = triage_all(findings)

    # Optional AI predictor attach
    try:
        from modules.ai.predictor import predict_findings
        findings = predict_findings(findings)
        logger.info("AI predictions attached to findings (count=%s)", len(findings))
    except Exception:
        logger.debug("No AI predictor available or failed", exc_info=True)

    # generate PoCs (non-destructive)
    logger.info("Generating non-destructive PoCs")
    pocs: List[Dict[str, Any]] = []
    try:
        pocs = await generate_pocs_for_findings(outdir, findings)
    except Exception:
        logger.exception("generate_pocs_for_findings failed")
    logger.info("PoCs generated: %s", len(pocs))

    # Write base reports
    meta = {
        "passive": passive,
        "active": active,
        "crawl_count": len(urls),
        "fingerprint": fp,
        "pocs_summary": {"count": len(pocs)},
        "pocs_file": "reports/pocs.json",
    }
    Reporter.write_reports(outdir, meta, findings)
    logger.info("Reports written")

    # Post-processing pipeline (normalize, map, attach, curated)
    #  - normalize_pocs: tolerant call
    try:
        called = safe_call_module_fn(normalize_pocs_module, ["normalize_pocs", "normalize", "main"], outdir)
        if called is None:
            logger.info("normalize_pocs: no known entrypoint found; skipping normalization (module present but no callable found).")
        else:
            logger.info("normalize_pocs executed")
    except Exception:
        logger.exception("normalize_pocs invocation failed")

    # map pocs
    try:
        mapped_path = safe_call_module_fn(map_pocs_module, ["map_pocs", "main"], outdir) or map_pocs_module.map_pocs(outdir)
        logger.info("map_pocs result: %s", mapped_path)
    except Exception:
        # try direct call if name differs
        try:
            mapped_path = map_pocs_module.map_pocs(outdir)
            logger.info("map_pocs result (direct): %s", mapped_path)
        except Exception:
            logger.exception("map_pocs_to_findings failed")

    # attach pocs
    try:
        attach_summary = safe_call_module_fn(attach_pocs_module, ["attach_pocs_to_report", "attach_pocs"], outdir, best_only=poc_best_only, threshold=poc_threshold)
        if attach_summary is None:
            # maybe attach_pocs.py is script-like; try import style attach_pocs_to_report in module
            attach_summary = attach_pocs_module.attach_pocs_to_report(outdir, best_only=poc_best_only, threshold=poc_threshold)
        logger.info("attach_pocs summary: %s", attach_summary)
    except Exception:
        logger.exception("attach_pocs failed")

    # generate curated report via dynamic module (tolerant)
    def _call_generate_curated_module(gen_mod, outdir, export_html=True, export_pdf=True, logger=None):
        """
        Call the loaded generate_curated module in a robust way:
        - try common entrypoints ('main', 'generate', 'run')
        - try calling without args (module reads sys.argv)
        - ensure sys.argv is restored afterwards
        """
        if logger is None:
            import logging as _logging
            logger = _logging.getLogger(__name__)

        # build argv for the module (used if the module uses argparse inside main())
        argv_backup = sys.argv[:]
        try:
            sys.argv = ["generate_curated.py", outdir]
            if export_html:
                sys.argv.append("--html")
            if export_pdf:
                sys.argv.append("--pdf")

            # try several common entrypoints
            for candidate in ("main", "generate", "run"):
                fn = getattr(gen_mod, candidate, None)
                if callable(fn):
                    try:
                        res = None
                        try:
                            res = fn()
                        except TypeError:
                            res = fn(outdir)
                        logger.info("Called generate_curated.%s successfully", candidate)
                        return res
                    except Exception as e:
                        logger.debug("generate_curated.%s raised, trying next: %s", candidate, e)

            # last-resort: try executing module-level 'main' by name if available via sys.argv
            if hasattr(gen_mod, "main") and callable(getattr(gen_mod, "main")):
                try:
                    return gen_mod.main()
                except Exception as e:
                    logger.debug("generate_curated.main() failed as fallback: %s", e)

            # nothing worked
            logger.warning("generate_curated module did not expose a usable entrypoint; skipping curated export.")
            return None
        finally:
            sys.argv = argv_backup

    # use the helper above where you used to call gen_mod
    try:
        if gen_mod:
            _call_generate_curated_module(gen_mod, outdir, export_html=export_html, export_pdf=export_pdf, logger=logger)
        else:
            logger.info("generate_curated module not available; skipping curated export.")
    except Exception as e:
        logger.exception("generate_curated failed: %s", e)

    logger.info("Non-destructive phase complete")
    return {
        "pocs_generated": len(pocs),
        "reports_dir": os.path.join(outdir, "reports"),
        "findings_count": len(findings),
    }

async def run_destructive_phase(scope: ScopeManager, outdir: str, logger=None, force: bool = False) -> dict:
    """
    Run destructive actions. This function will call destructive adapters only when
    `scope.is_destructive_allowed(outdir)` returns True OR when force=True.
    Returns a summary dict and merges findings into a new report file.
    """
    if logger is None:
        logger = init_logger(outdir)
    logger.warning("Starting DESTRUCTIVE phase — verifying safeguards")

    if not force and not scope.is_destructive_allowed(outdir):
        logger.error("Destructive safeguards not satisfied. Aborting destructive phase.")
        return {"destructive_results": [], "merged_report": None, "skipped": True}

    results: List[Dict[str, Any]] = []

    # run sqlmap exploit wrapper (if present)
    try:
        from modules.destructive.sqlmap_exploit import run_sqlmap_exploit
        s = await run_sqlmap_exploit(scope, outdir)
        results.append({"module": "sqlmap_exploit", "results": s})
        logger.info("sqlmap_exploit returned %s items", len(s))
    except Exception as e:
        logger.exception("sqlmap_exploit error: %s", e)
        results.append({"module": "sqlmap_exploit", "error": str(e)})

    # optional RCE tester (if present)
    try:
        from modules.destructive.rce_tester import run_rce_tests
        rce_findings = await run_rce_tests(scope, outdir)
        if rce_findings:
            results.append({"module": "rce_tester", "results": rce_findings})
            logger.info("rce_tester returned %s findings", len(rce_findings))
            # save raw
            genp = os.path.join(outdir, "generated", "rce_tester.json")
            write_json(genp, {"result": rce_findings})
    except Exception as e:
        logger.exception("rce_tester invocation failed: %s", e)
        # don't abort, just record the error
        results.append({"module": "rce_tester", "error": str(e)})

    # Merge destructive findings into an existing final report (create if needed)
    out_path = None
    try:
        rpt_candidates = [
            os.path.join(outdir, "reports", "final_report_with_pocs_map.json"),
            os.path.join(outdir, "reports", "final_report_with_pocs.json"),
            os.path.join(outdir, "reports", "final_report.json"),
        ]
        final = None
        base_report_path = None
        for c in rpt_candidates:
            if os.path.isfile(c):
                try:
                    with open(c, "r", encoding="utf-8") as fh:
                        final = json.load(fh)
                    base_report_path = c
                    break
                except Exception:
                    final = None
        if final is None:
            final = {"findings": [], "meta": {}}
            base_report_path = os.path.join(outdir, "reports", "final_report.json")

        existing_evidence = {
            (f.get("type"), f.get("target"), str(f.get("evidence"))[:200]) for f in final.get("findings", [])
        }
        appended = 0
        for f in results:
            key = (f.get("module"), None, str(f)[:200])
            # simple dedupe: avoid appending identical module-result blobs
            if key not in existing_evidence:
                final.setdefault("findings", []).append(f)
                appended += 1

        final_meta = final.get("meta", {})
        final_meta.setdefault("destructive", {})["appended_count"] = appended
        final_meta["destructive"]["source_report"] = os.path.relpath(base_report_path, outdir)
        final["meta"] = final_meta

        out_path = os.path.join(outdir, "reports", "final_report_with_destructive.json")
        write_json(out_path, final)
        logger.info("Wrote merged destructive report: %s (appended=%s)", out_path, appended)
    except Exception as e:
        logger.exception("Failed to merge destructive findings into report: %s", e)

    logger.warning("Destructive phase complete")
    return {"destructive_results": results, "merged_report": out_path if out_path else None}

async def run(scope: ScopeManager, outdir: str,
              poc_best_only: bool = True, poc_threshold: float = 0.5,
              export_html: bool = True, export_pdf: bool = True,
              run_destructive: bool = False, force_destructive: bool = False):
    logger = init_logger(outdir)
    nd_summary = await run_non_destructive_phase(scope, outdir,
                                                 poc_best_only=poc_best_only,
                                                 poc_threshold=poc_threshold,
                                                 export_html=export_html,
                                                 export_pdf=export_pdf)

    if run_destructive:
        # respects force_destructive flag
        if not force_destructive and not scope.is_destructive_allowed(outdir):
            logger.error("Destructive mode blocked by safeguards (env/proof). Not running destructive phase.")
        else:
            logger.warning("Destructive safeguards passed or forced — running destructive phase")
            d_summary = await run_destructive_phase(scope, outdir, logger=logger, force=force_destructive)
            logger.info("Destructive summary: %s", d_summary)
    else:
        logger.info("Destructive phase not requested; skipping.")

    logger.info("Run orchestration complete")
    return {"non_destructive": nd_summary}

def main():
    p = argparse.ArgumentParser()
    p.add_argument("--targets", nargs="+", required=False,
                   default=["https://testphp.vulnweb.com"])
    p.add_argument("--run-id", default="run01")
    p.add_argument("--mode", choices=["non-destructive", "destructive"],
                   default="non-destructive")
    p.add_argument("--poc-multi", action="store_true",
                   help="Attach each PoC to multiple findings (if threshold met). Default: single best match")
    p.add_argument("--poc-threshold", type=float, default=0.5,
                   help="Minimum match score [0..1] for attaching PoC (default 0.5)")
    p.add_argument("--no-html", action="store_true", help="Skip HTML report")
    p.add_argument("--no-pdf", action="store_true", help="Skip PDF report")
    p.add_argument("--force-destructive", action="store_true",
                   help="Bypass destructive safeguards and run destructive modules (use with --mode destructive).")
    args = p.parse_args()

    scope = ScopeManager(args.targets, mode=args.mode)
    outdir = f"runs/{scope.primary_domain}/{args.run_id}"
    scope.prepare_workspace(outdir)

    run_destructive = (args.mode == "destructive")
    force = bool(args.force_destructive)

    try:
        asyncio.run(run(scope, outdir,
                        poc_best_only=not args.poc_multi,
                        poc_threshold=args.poc_threshold,
                        export_html=not args.no_html,
                        export_pdf=not args.no_pdf,
                        run_destructive=run_destructive,
                        force_destructive=force))
        print(f"Run complete. Reports at {outdir}/reports")
    except KeyboardInterrupt:
        print("Interrupted by user")
        sys.exit(1)

if __name__ == "__main__":
    main()
